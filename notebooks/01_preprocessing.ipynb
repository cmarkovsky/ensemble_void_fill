{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7a0e9cf-6c66-421d-bac8-fb2d86686b22",
   "metadata": {},
   "source": [
    "# Ensemble Machine Learning for Void Filling in Glacier Elevation Change Maps\n",
    "*By Cameron Markovsky*\n",
    "## 01 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ab0e0b-7a1a-42d4-bad7-c5797fbe0380",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7aded788-4741-4f54-8c5d-93127af25684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55178845-1ba0-421a-ab96-c766cdc73000",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16faf109-7f05-407d-9e23-f1948c6d0899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path and load the Eastern Himalaya data\n",
    "ehim_path = './data/HIMAP_RGI/ehim_mb6.shp' \n",
    "ehim = gpd.read_file(ehim_path)\n",
    "\n",
    "# Set the path and load the Western Himalaya data\n",
    "whim_path = './data/HIMAP_RGI/whim_mb6.shp'\n",
    "whim = gpd.read_file(whim_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704ea817-2068-490a-b473-59c973095c4d",
   "metadata": {},
   "source": [
    "### Create the DatasetBuilder Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47ff7dbb-326e-45cf-b48b-6f01fc6c4d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data = False # Set to true if you would like to save new copies of the data\n",
    "\n",
    "class DatasetBuilder:\n",
    "    \"\"\"\n",
    "    Attributes:\n",
    "        dict (dict): Dictionary mapping region names to pandas DataFrames containing glacier data.\n",
    "        surge_path (str): File path to the surge glacier CSV file.\n",
    "        dc_path (str): File path to the debris-cover ratio CSV file.\n",
    "        save (bool): Flag indicating whether to save processed datasets to disk.\n",
    "        surge (pd.DataFrame): DataFrame containing surge glacier information.\n",
    "        dc (pd.DataFrame): DataFrame containing debris-cover ratio information.\n",
    "        \n",
    "    Methods:\n",
    "        __init__(region_dict, save=False)\n",
    "            Initializes the DatasetBuilder with region data and loads auxiliary datasets.\n",
    "        iter_regs()\n",
    "            Iterates over all regions in the dictionary, processes each DataFrame,\n",
    "            and optionally saves the processed dataset to a CSV file.\n",
    "        _build_ds_from_df(c_df)\n",
    "            Processes a single region DataFrame by trimming columns, filtering surge glaciers,\n",
    "            adding debris-cover ratio, computing hypsometric index, and converting aspect.\n",
    "        _load_surge_debris()\n",
    "            Loads the surge glacier and debris-cover ratio datasets from their respective CSV files.\n",
    "        _trim_df(reg_df)\n",
    "            Selects and renames relevant columns from the input DataFrame for further processing.\n",
    "        _filter_surge(reg_df)\n",
    "            Removes glaciers identified as surge-type from the input DataFrame.\n",
    "        _add_dc(reg_df)\n",
    "            Joins the debris-cover ratio data to the input DataFrame based on glacier IDs.\n",
    "        _add_HI(reg_df)\n",
    "            Computes the hypsometric index (HI) for each glacier.\n",
    "        _conv_aspect(reg_df)\n",
    "            Decomposes the 'Aspect' column into sine and cosine components.\n",
    "    \"\"\"\n",
    "    def __init__(self, region_dict, save = save_data):\n",
    "        self.dict = region_dict\n",
    "        self.surge_path = \"./data/other/surge_glaciers.csv\"\n",
    "        self.dc_path = \"./data/other/dc_ratio.csv\"\n",
    "        self.regs = {}\n",
    "        self.save = save\n",
    "        self._load_surge_debris()\n",
    "        self.iter_regs()\n",
    "        \n",
    "    def iter_regs(self):\n",
    "        for reg in self.dict.keys():\n",
    "            c_df = self._build_ds_from_df(self.dict[reg])\n",
    "            self.regs[reg] = c_df\n",
    "            if self.save: c_df.to_csv(f\"./data/raw/{reg}_preprocessed.csv\", index = False)\n",
    "\n",
    "    def _build_ds_from_df(self, c_df):\n",
    "        c_df = self._trim_df(c_df)\n",
    "        c_df = self._filter_surge(c_df)\n",
    "        c_df = self._add_dc(c_df)\n",
    "        c_df = self._add_HI(c_df)\n",
    "        c_df = self._conv_aspect(c_df)\n",
    "        return c_df\n",
    "\n",
    "    def _load_surge_debris(self):\n",
    "        self.surge = pd.read_csv(self.surge_path)\n",
    "        self.dc = pd.read_csv(self.dc_path)\n",
    "\n",
    "    def _trim_df(self, reg_df):\n",
    "        cols = ['RGIId', 'GLIMSId','CenLon', 'CenLat', 'Area', 'Zmin', 'Zmax', 'Zmed', 'Slope', 'Aspect', 'dhdt_ma', \n",
    "        'dhdt_ma_si', 'mb_mwea', 'mb_mwea_si', 'RGIId_x']\n",
    "        new_df = reg_df[cols].copy()\n",
    "        new_df.rename(columns = {'RGIId_x': 'RGIId_Full'}, inplace = True)\n",
    "        return new_df\n",
    "        \n",
    "    def _filter_surge(self, reg_df):\n",
    "        reg_ids = reg_df['GLIMSId']\n",
    "        surge_ids = self.surge['Glac_ID']\n",
    "        overlap_ids = set(reg_ids).intersection(surge_ids)\n",
    "\n",
    "        if len(overlap_ids) == 0:\n",
    "            return reg_df\n",
    "        else:\n",
    "            reg_df_no_overlap = reg_df[~reg_df['GLIMSId'].isin(overlap_ids)]\n",
    "            return reg_df_no_overlap\n",
    "\n",
    "\n",
    "    def _add_dc(self, reg_df):\n",
    "        new_df = reg_df.set_index('RGIId_Full').join(self.dc.set_index('RGIId')['dc_ratio'], lsuffix = '')\n",
    "        new_df = new_df.reset_index()\n",
    "        return new_df\n",
    "\n",
    "    def _add_HI(self, reg_df):\n",
    "        reg_df['HI'] = (reg_df['Zmax'] - reg_df['Zmed']) / (reg_df['Zmed'] - reg_df['Zmin'])\n",
    "        reg_df['HI'] = reg_df['HI'].apply(lambda x: -1 / x if x <1 else x)\n",
    "        return reg_df\n",
    "\n",
    "    def _conv_aspect(self, reg_df):\n",
    "        aspect = reg_df['Aspect']\n",
    "        aspect2 = np.deg2rad(reg_df['Aspect'])\n",
    "        reg_df['sin_Aspect'], reg_df['cos_Aspect'] = np.sin(aspect2), np.cos(aspect2)\n",
    "        return reg_df\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f995bfe-9c62-4cb6-ab30-fcac623dd999",
   "metadata": {},
   "source": [
    "### Instantiate DatasetBuilder object and iterate through each region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6777b30f-c153-4811-b5de-f8bff03172b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = {'ehim': ehim, 'whim': whim} # Create a dictionary of the regions of interest\n",
    "\n",
    "db = DatasetBuilder(regs) # Instantiate the DatasetBuilder class with the dictionary\n",
    "db.iter_regs() # Iterate through each region and perform pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bbd0ee-eefd-46ea-9e49-b079b92eb0ac",
   "metadata": {},
   "source": [
    "### Examine the preprocessed datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9bc4a588-35ca-42d2-83fd-6ced2277089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ehim_preprocessed = db.regs['ehim'] # Preprocessed Eastern Himalaya Data\n",
    "whim_preprocessed = db.regs['whim'] # Preprocessed Western Himalaya Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

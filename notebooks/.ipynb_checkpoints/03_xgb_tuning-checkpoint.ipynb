{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f948d67-4795-4955-af39-5621bd1029a3",
   "metadata": {},
   "source": [
    "# Ensemble Machine Learning for Void Filling in Glacier Elevation Change Maps\n",
    "*By Cameron Markovsky*\n",
    "\n",
    "## 03 - XGB Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8770af17-cc00-4315-8e9c-1a1afb038fbc",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5421e44-5ecc-4db4-8cea-8f5f25905f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42264dc6-bcd7-4198-9a18-2364e20ddb84",
   "metadata": {},
   "source": [
    "### Define DataPrepper Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2e62034-d024-4f72-a731-ed2f2edc9783",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPrepper:\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.data = self.load_data()\n",
    "        self.train, self.test = self.preprocess_data()\n",
    "        # self.X = pd.concat([self.X_train, self.X_test])\n",
    "        # self.y = pd.concat([self.y_train, self.y_test])\n",
    "        \n",
    "        # self.train = self.train.drop(columns = ['RGIId_Full', 'void_mask'])\n",
    "        # self.test = self.test.drop(columns = ['RGIId_Full', 'void_mask'])\n",
    "        # self.full = self.data.drop(columns = ['RGIId_Full', 'void_mask'])\n",
    "        # self.X = self.X.drop(columns = ['RGIId_Full', 'target', 'void_mask'])\n",
    "\n",
    "\n",
    "    def load_data(self):\n",
    "        try:\n",
    "            data = pd.read_csv(self.filename)\n",
    "        except:\n",
    "            data = pd.read_feather(self.filename)\n",
    "        try:\n",
    "            data = data.drop(columns=['Unnamed: 0'])\n",
    "        except:\n",
    "            pass\n",
    "        data = data.rename(columns={'dh1': 'target', 'elevation': 'z'})\n",
    "        data = data.dropna()\n",
    "        return data\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        train = self.data[self.data['void_mask'] == False]\n",
    "        # y_train = x_train['target']\n",
    "        \n",
    "        test = self.data[self.data['void_mask'] == True]\n",
    "        # y_test = x_test['target']\n",
    "        return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4db60c2-ab3f-4b56-b59c-f602328b6cfb",
   "metadata": {},
   "source": [
    "### Define global parameters and Config Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d53d80f4-ef0c-41c7-ba8e-3ef81de06b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../data/ts1.csv' # name of file to tune XGB on\n",
    "n_trials = 10 # number of Optuna trials to run\n",
    "save_cfg = True # save the model configuration?\n",
    "\n",
    "class config:\n",
    "\n",
    "    filename = filename\n",
    "    # filename = 'data/ts1.csv'\n",
    "    if filename == '../data/ts1.csv':\n",
    "        savename = 'ts'\n",
    "    elif filename == 'data/ehim_full.csv':\n",
    "        savename = 'ehim'\n",
    "    elif filename == 'data/whim_full.csv':\n",
    "        savename = 'whim'\n",
    "    else:\n",
    "        savename = 'other'\n",
    "\n",
    "    dp = DataPrepper(filename)\n",
    "    train, test = dp.train, dp.test\n",
    "    full = pd.concat([train, test])\n",
    "    # tune_train, tune_test = train_test_split(full, test_size=0.37, random_state=42)\n",
    "    features = ['x', 'y', 'z', 'Area', 'Zmin', 'Zmed', 'Zmax', 'Slope', 'dc_ratio', 'HI', 'sin_Aspect', 'cos_Aspect']\n",
    "    target = 'target'\n",
    "    n_rounds = 1\n",
    "    n_trials = n_trials\n",
    "    save_cfg = save_cfg\n",
    "\n",
    "    def save_cfg(self, filename):\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747f3456-7f56-4707-b216-2593ad30d341",
   "metadata": {},
   "source": [
    "### Define the XGB Objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c39d890d-59da-4bde-8c9b-6569197fc3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_xgb(trial):\n",
    "    n = trial.number\n",
    "    validation, test_valid = train_test_split(config.test, test_size=0.50)\n",
    "\n",
    "    # Suggest values of the hyperparameters using a trial object.\n",
    "    params = {\n",
    "        \"objective\": 'reg:squarederror',\n",
    "        'tree_method': \"hist\",\n",
    "        # \"device\": \"cuda\",\n",
    "        'lambda': trial.suggest_float('lambda', 1e-3, 100.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-3, 100.0, log=True),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1e-3, 100, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 20),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.3, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.3, 1.0),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 200),\n",
    "    }\n",
    "\n",
    "    # X_train, y_train = CFG.tune_train[CFG.features], CFG.tune_train[CFG.target] # Random split of 63% train, 37% test\n",
    "    # X_test, y_test = CFG.tune_test[CFG.features], CFG.tune_test[CFG.target] # Random split of 63% train, 37% test\n",
    "\n",
    "    train = config.train\n",
    "    # test = CFG.test_valid\n",
    "\n",
    "    X_train, y_train = train[config.features], train[config.target]\n",
    "    X_valid, y_valid = validation[config.features], validation[config.target]\n",
    "    # X_test, y_test = test[CFG.features], test[CFG.target]\n",
    "\n",
    "    dtrain = xgb.DMatrix(data=X_train, label=y_train)\n",
    "    dvalidation = xgb.DMatrix(data=X_valid, label=y_valid)\n",
    "    # dtest = xgb.DMatrix(data=X_test, label=y_test)\n",
    "    \n",
    "    model_xgb = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            evals=[(dvalidation, 'eval')],\n",
    "            num_boost_round=1000,\n",
    "            early_stopping_rounds=50,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "    # model = xgb.XGBRegressor(**params)\n",
    "    # model.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n",
    "    y_preds_xgb = model_xgb.predict(dvalidation)\n",
    "\n",
    "    rmse = root_mean_squared_error(y_valid, y_preds_xgb)\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889fd30e-836f-404f-8958-de5e903d211f",
   "metadata": {},
   "source": [
    "### Define methods to run Optuna optimization and retrieve the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "425ba74b-a33d-4fd2-a6ef-e370a7ad1a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_xgb_optuna(n_trials=10):\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective_xgb, n_trials=n_trials)\n",
    "    return study, study.best_params, study.best_value\n",
    "\n",
    "def get_xgb_params(best_params):\n",
    "    xgb_params = {\n",
    "        \"objective\": 'reg:squarederror',\n",
    "        # \"tree_method\": \"hist\",\n",
    "        # \"device\": \"cuda\",\n",
    "        # \"num_boosted_round\": 1000, #trial.suggest_int(\"n_estimators\", 1, 2000),\n",
    "        # \"early_stopping_rounds\": 50, #100\n",
    "        'lambda': best_params['lambda'], \n",
    "        'alpha': best_params['alpha'], \n",
    "        'learning_rate': best_params['learning_rate'], \n",
    "        'gamma': best_params['gamma'], \n",
    "        'max_depth': best_params['max_depth'], \n",
    "        'subsample': best_params['subsample'], \n",
    "        'colsample_bytree': best_params['colsample_bytree'], \n",
    "        'min_child_weight': best_params['min_child_weight']\n",
    "    }\n",
    "    return xgb_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a719a352-de68-41bb-a444-1cd0ce649dc4",
   "metadata": {},
   "source": [
    "# Run the Optuna optimiziation and save the configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ddac9f1-f60b-417f-b806-cab525c67325",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-10 11:01:51,953] A new study created in memory with name: no-name-28e07a06-4ac2-487c-add0-8102d1d95b0c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing XGBoost hyperparameters on ts1.csv using 10 trials\n",
      "Train size: 1786; Test size: 1042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-10 11:01:52,497] Trial 0 finished with value: 0.47485789707060083 and parameters: {'lambda': 1.556981142447684, 'alpha': 0.012530083882374227, 'learning_rate': 0.03926153881790991, 'gamma': 0.16497599858860146, 'max_depth': 6, 'subsample': 0.5230790731182884, 'colsample_bytree': 0.8212602748454507, 'min_child_weight': 64}. Best is trial 0 with value: 0.47485789707060083.\n",
      "[I 2025-06-10 11:01:52,746] Trial 1 finished with value: 0.5054362065940325 and parameters: {'lambda': 0.026033885354085944, 'alpha': 0.5819617905285948, 'learning_rate': 0.06684033567529597, 'gamma': 0.7219480423536395, 'max_depth': 13, 'subsample': 0.471313027491404, 'colsample_bytree': 0.8323267100189169, 'min_child_weight': 72}. Best is trial 0 with value: 0.47485789707060083.\n",
      "[I 2025-06-10 11:01:53,414] Trial 2 finished with value: 0.5197453618356351 and parameters: {'lambda': 0.029692497962988052, 'alpha': 0.23394400008976124, 'learning_rate': 0.00508807735372225, 'gamma': 0.3295702032078926, 'max_depth': 10, 'subsample': 0.3291705073552064, 'colsample_bytree': 0.39892600203959067, 'min_child_weight': 131}. Best is trial 0 with value: 0.47485789707060083.\n",
      "[I 2025-06-10 11:01:53,684] Trial 3 finished with value: 0.562045983461349 and parameters: {'lambda': 0.028247421562638472, 'alpha': 0.034871358462794974, 'learning_rate': 0.009813575379934755, 'gamma': 13.877276480897258, 'max_depth': 6, 'subsample': 0.3085506224450105, 'colsample_bytree': 0.4433360255484571, 'min_child_weight': 126}. Best is trial 0 with value: 0.47485789707060083.\n",
      "[I 2025-06-10 11:01:53,888] Trial 4 finished with value: 0.47514970457169575 and parameters: {'lambda': 0.0064462938185401095, 'alpha': 0.00905025696103339, 'learning_rate': 0.016978228847122634, 'gamma': 3.0070542626354735, 'max_depth': 7, 'subsample': 0.44216402305364577, 'colsample_bytree': 0.8026766065606485, 'min_child_weight': 32}. Best is trial 0 with value: 0.47485789707060083.\n",
      "[I 2025-06-10 11:01:54,230] Trial 5 finished with value: 0.5238370403655636 and parameters: {'lambda': 0.27038520617630457, 'alpha': 0.007700072962097263, 'learning_rate': 0.011799348744338238, 'gamma': 0.08285485690877645, 'max_depth': 6, 'subsample': 0.319692931079343, 'colsample_bytree': 0.6242593361857798, 'min_child_weight': 161}. Best is trial 0 with value: 0.47485789707060083.\n",
      "[I 2025-06-10 11:01:54,897] Trial 6 finished with value: 0.5165135934634361 and parameters: {'lambda': 0.0023798259239977333, 'alpha': 0.009458249436369014, 'learning_rate': 0.006198400035359732, 'gamma': 0.12472055596591664, 'max_depth': 18, 'subsample': 0.6104770731602811, 'colsample_bytree': 0.46058650776729104, 'min_child_weight': 66}. Best is trial 0 with value: 0.47485789707060083.\n",
      "[I 2025-06-10 11:01:55,084] Trial 7 finished with value: 0.6471182804752738 and parameters: {'lambda': 2.3923739618985405, 'alpha': 8.373556752376794, 'learning_rate': 0.008768680083135609, 'gamma': 24.971057518326774, 'max_depth': 10, 'subsample': 0.3560442489413098, 'colsample_bytree': 0.5392249197457455, 'min_child_weight': 37}. Best is trial 0 with value: 0.47485789707060083.\n",
      "[I 2025-06-10 11:01:55,415] Trial 8 finished with value: 0.5074780218946014 and parameters: {'lambda': 0.4606971525893731, 'alpha': 20.126852025352786, 'learning_rate': 0.0479954460050679, 'gamma': 0.017138377433655577, 'max_depth': 15, 'subsample': 0.6181305886030273, 'colsample_bytree': 0.41786910295119384, 'min_child_weight': 109}. Best is trial 0 with value: 0.47485789707060083.\n",
      "[I 2025-06-10 11:01:55,527] Trial 9 finished with value: 0.5338164092928983 and parameters: {'lambda': 53.18350188473703, 'alpha': 0.16282991534518101, 'learning_rate': 0.03093851297734486, 'gamma': 17.01964679453678, 'max_depth': 18, 'subsample': 0.5382745451868001, 'colsample_bytree': 0.6323042843112043, 'min_child_weight': 115}. Best is trial 0 with value: 0.47485789707060083.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGB Parameters: {'lambda': 1.556981142447684, 'alpha': 0.012530083882374227, 'learning_rate': 0.03926153881790991, 'gamma': 0.16497599858860146, 'max_depth': 6, 'subsample': 0.5230790731182884, 'colsample_bytree': 0.8212602748454507, 'min_child_weight': 64}\n",
      "-----------------------------------\n",
      "Finished optimizing XGBoost hyperparameters.\n",
      "\n",
      "Saved model configuration to ../models/ts_cfg.pkl\n"
     ]
    }
   ],
   "source": [
    "cfg = config()\n",
    "\n",
    "\n",
    "print(f'Optimizing XGBoost hyperparameters on {filename.split('/')[2]} using {n_trials} trials')\n",
    "print(f'Train size: {len(cfg.train)}; Test size: {len(cfg.test)}')\n",
    "study, best_params, best_rmse = run_xgb_optuna(n_trials)\n",
    "xgb_params = get_xgb_params(best_params)\n",
    "config.xgb_params = xgb_params\n",
    "print(f'Best XGB Parameters: {best_params}')\n",
    "print('-----------------------------------')\n",
    "print('Finished optimizing XGBoost hyperparameters.\\n')\n",
    "if save_cfg:\n",
    "    cfg.save_cfg(f'../models/{cfg.savename}_cfg.pkl')\n",
    "print(f'Saved model configuration to ../models/{cfg.savename}_cfg.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13dde5b-5b3a-4848-823b-8c8c491c5e83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
